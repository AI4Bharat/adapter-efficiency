# A Comprehensive Analysis of Adapter Efficiency
This repository contains code for the paper A Comprehensive Analysis of Adapter Efficiency.
<br>
[IndicBert](https://github.com/AI4Bharat/adapter-efficiency/tree/main/IndicBert) contains code for Adapter tuning, Fine-tuning and multitask training for IndicBert model.
<br>
[XLMR](https://github.com/AI4Bharat/adapter-efficiency/tree/main/XLMR)  contains code for Adapter tuning, Fine-tuning ans multitask training for XLMR-Base and XLMR-Large model.
<br>
# Contributors
Nandini Mundra  (cs21s041@cse.iitm.ac.in)
<br>
Sumanth Doddapaneni
<br>
Raj Dabre
<br>
Anoop Kunchukuttan
<br>
Ratish Puduppully
<br>
Mitesh M. Khapra
<br>
# LICENSE
code will be release under an [MIT license](https://github.com/ai4bharat/IndicBERT/blob/main/LICENSE)
# Acknowledgement
We would like to thank the Ministry of Electronics and Information Technology https://www.meity.gov.in/ of the Government of India for their generous grant through the Digital India Bhashini project https://www.bhashini.gov.in/ . We also thank the Centre for Development of Advanced Computing  https://www.cdac.in/index.aspx?id=pune  for providing compute time on the Param Siddhi Supercomputer. We also thank Nilekani Philanthropies for their generous grant towards building datasets, models, tools and resources for Indic languages. We also thank Microsoft for their grant to support research on Indic languages.
